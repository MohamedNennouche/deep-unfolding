{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#deep-unfolding","title":"deep unfolding","text":"<p>The package includes iterative methods for solving linear equations. However, due to the various parameters and performance of the iterative approach, it is necessary to optimize these parameters to improve the convergence rate. Such proposed tool called deep_unfolding, which takes an iterative algorithm with a fixed number of iterations T, unravels its structure, and adds trainable parameters. These parameters are then trained using deep learning techniques such as loss functions, stochastic gradient descent, and back-propagation.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install --upgrade pip\npip install deep_unfolding\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":"<p><pre><code>from deep_unfolding import Iterative\nfrom iterative import result\nlist_iterative=['RI', 'SOR', 'GS']\nresult(list_iterative)\n</code></pre> </p> <p><pre><code>from deep_unfolding import IterativeNet\nfrom IterativeNet import main\nlist_iterative=['RINet','RI', 'SORNet','SOR', 'GS']\nmain(list_iterative)\n</code></pre> </p>"},{"location":"#the-training-process","title":"The training process","text":""},{"location":"function/","title":"Description the code","text":""},{"location":"function/#initialization-value","title":"Initialization value","text":"<pre><code>## model parameters \nitr = 25 \n'''\n    iteration steps $T$\n'''\ntotal_itr = itr \n'''\n    max. iterations\n    '''\nn = 300 \nm = 600 \n'''\n    The size of matrix generation\n    '''\n##\n## training parameters\nbs = 200 # mini batch size\nnum_batch = 500 # number of mini batches\nlr_adam = 0.002 # learning rate of optimizer\ninit_val_SORNet = 1.1 # initial values of $\\omega$\ninit_val_SOR_CHEBY_Net_omega = 0.6 # initial values of $\\omega$\ninit_val_SOR_CHEBY_Net_gamma = 0.8 # initial values of $\\gamma$\ninit_val_SOR_CHEBY_Net_alpha = 0.9 # initial values of $\\alpha$\ninit_val_AORNet_r = 0.9 # initial values of $\\r$\ninit_val_AORNet_omega = 1.5 # initial values of $\\omega$\ninit_val_RINet = 0.1 # initial values of $\\omega$\n##\n## parameters for evauation of generalization error \ntotal_itr=25 # total number of iterations (multiple number of \"itr\")\nbs = 10000 # number of samples \n##\n# generate A and H\nseed_ = 12\nnp.random.seed(seed=seed_)\n'''\n    the random number to gneration the whole data\n    '''\nH = np.random.normal(0,1.0/math.sqrt(n),(n,m)) \nA = np.dot(H,H.T)\neig = np.linalg.eig(A)\neig = eig[0] # eigenvalues\nW = torch.Tensor(np.diag(eig)).to(device)\nH = torch.from_numpy(H).float().to(device)\nD = np.diag(np.diag(A))\n'''\n    the digonal matrix\n    '''\nU = np.triu(A, 1)\n'''\n    the model of SOR method\n    '''\nL = np.tril(A, -1)\n'''\n    the model of SOR method\n    '''\nDinv = np.linalg.inv(D)\n'''\n    the model of SOR method\n    '''\ninvM = np.linalg.inv(D +L)  \n'''\n    the model of SOR method\n    '''\n</code></pre>"},{"location":"function/#the-conventional-sor-with-optimum-in-value","title":"The conventional SOR with optimum in value","text":"<pre><code>   class SOR(nn.Module):\n'''\n    the model of SOR method\n    '''\ndef __init__(self, num_itr):\nsuper(SOR, self).__init__()\ndef forward(self, num_itr, bs,y):\ntraj = []\nomega = torch.tensor(1.2);     inv_omega = torch.div(1,omega)\n'''\n        the paramter of SOR $\\omega=1.2$\n        '''\ninvM_sor = torch.linalg.inv(D-torch.mul(inv_omega, L))  \ns  = torch.zeros(bs,n).to(device)\ntraj.append(s)                  \nyMF = y@H.T\ns = torch.matmul( yMF, Dinv)   \n'''\n        Generate solution vevtor\n        '''                                      \nfor i in range(num_itr):\ntemp = torch.mul((inv_omega-1),D)+ torch.mul((inv_omega),U)\ns = torch.matmul(s, torch.matmul(invM_sor,temp))+torch.matmul(yMF, invM_sor)\n'''\n            The s final solotion\n             ''' \ntraj.append(s) \nreturn s, traj\nsor_model = SOR(itr).to(device)\n</code></pre>"},{"location":"function/#the-deep-unfolding-of-sor-as-sornet","title":"The deep unfolding of  SOR , as SORNet","text":"<pre><code>## Deep unfolded SOR with a constant step size \nclass SORNet(nn.Module):\ndef __init__(self, num_itr):\nsuper(SORNet, self).__init__()\nself.inv_omega = nn.Parameter(init_val_SORNet*torch.ones(1))\n'''\n        the parameter of SORNet $\\omega_t$ input to the Deep unfolded to the optimal value\n        '''\ndef forward(self, num_itr, bs,y):\ntraj = []\ninvM = torch.linalg.inv(torch.mul(self.inv_omega[0], D) +L) \n#invM_sor = torch.linalg.inv(D-torch.mul(self.inv_omega[0], L)) \ns  = torch.zeros(bs,n).to(device)\ntraj.append(s)                  \nyMF = y@H.T\ns = torch.matmul( yMF, Dinv)                                           # Generate batch initial solution vevtor\nfor i in range(num_itr):\n#temp = torch.mul((self.inv_omega[0]-1),D)+ torch.mul((self.inv_omega[0]),U)\n#s = torch.matmul(s, torch.matmul(invM_sor,temp))+torch.matmul(yMF, invM_sor)\ntemp = torch.matmul(s, torch.mul((self.inv_omega[0]-1),D)-U)+ yMF\ns = torch.matmul(temp,invM) \ntraj.append(s) \nreturn s, traj\nmodel_SorNet = SORNet(itr).to(device)\nloss_func = nn.MSELoss()\nopt1   = optim.Adam(model_SorNet.parameters(), lr=lr_adam)\n</code></pre>"},{"location":"function/#the-training-process-of-sornet","title":"The training process of SORNet","text":"<pre><code>loss_gen=[]\nfor gen in (range(itr)): # incremental training\nfor i in range(num_batch):\nopt1.zero_grad()\nsolution = torch.normal(0.0*torch.ones(bs,n),1.0).to(device)\ny = solution @ H\nx_hat,_ = model_SorNet(gen + 1, bs,y)\nloss  = loss_func(x_hat, solution)\nloss.backward()\nopt1.step()\nif i % 200 == 0:\nprint(\"generation:\",gen+1, \" batch:\",i, \"\\t MSE loss:\",loss.item() )\nloss_gen.append(loss.item())\n</code></pre>"},{"location":"function/#the-loss-function","title":"The Loss function","text":"<pre><code>generation: 1  batch: 0      MSE loss: 0.21193785965442657\ngeneration: 1  batch: 200    MSE loss: 0.16124731302261353\ngeneration: 1  batch: 400    MSE loss: 0.15413512289524078\ngeneration: 2  batch: 0      MSE loss: 0.10527851432561874\ngeneration: 2  batch: 200    MSE loss: 0.09313789755105972\ngeneration: 2  batch: 400    MSE loss: 0.07886634021997452\ngeneration: 3  batch: 0      MSE loss: 0.043511394411325455\ngeneration: 3  batch: 200    MSE loss: 0.03456632420420647\ngeneration: 3  batch: 400    MSE loss: 0.032005008310079575\ngeneration: 4  batch: 0      MSE loss: 0.012704577296972275\ngeneration: 4  batch: 200    MSE loss: 0.01162696722894907\ngeneration: 4  batch: 400    MSE loss: 0.013365212827920914\ngeneration: 5  batch: 0      MSE loss: 0.0043519423343241215\ngeneration: 5  batch: 200    MSE loss: 0.004235091619193554\ngeneration: 5  batch: 400    MSE loss: 0.004391568712890148\ngeneration: 6  batch: 0      MSE loss: 0.0018134030979126692\ngeneration: 6  batch: 200    MSE loss: 0.0015237266197800636\ngeneration: 6  batch: 400    MSE loss: 0.001996097154915333\ngeneration: 7  batch: 0      MSE loss: 0.0007896940805949271\ngeneration: 7  batch: 200    MSE loss: 0.0007846651133149862\ngeneration: 7  batch: 400    MSE loss: 0.000536845822352916\ngeneration: 8  batch: 0      MSE loss: 0.00029308913508430123\ngeneration: 8  batch: 200    MSE loss: 0.00028003635816276073\ngeneration: 8  batch: 400    MSE loss: 0.0002928390458691865\ngeneration: 9  batch: 0      MSE loss: 0.00010200320684816688\ngeneration: 9  batch: 200    MSE loss: 0.00010443529754411429\ngeneration: 9  batch: 400    MSE loss: 9.555269207339734e-05\ngeneration: 10  batch: 0     MSE loss: 4.350042945588939e-05\ngeneration: 10  batch: 200   MSE loss: 4.744587931782007e-05\ngeneration: 10  batch: 400   MSE loss: 3.300118987681344e-05\ngeneration: 11  batch: 0     MSE loss: 1.364766103506554e-05\ngeneration: 11  batch: 200   MSE loss: 1.927423545566853e-05\ngeneration: 11  batch: 400   MSE loss: 1.7909926100401208e-05\ngeneration: 12  batch: 0     MSE loss: 5.9777075875899754e-06\ngeneration: 12  batch: 200   MSE loss: 6.701727215840947e-06\ngeneration: 12  batch: 400   MSE loss: 7.5510765782382805e-06\ngeneration: 13  batch: 0     MSE loss: 2.803974439302692e-06\ngeneration: 13  batch: 200   MSE loss: 2.259379471070133e-06\ngeneration: 13  batch: 400   MSE loss: 2.2352169253281318e-06\ngeneration: 14  batch: 0     MSE loss: 1.5446163388332934e-06\ngeneration: 14  batch: 200   MSE loss: 1.128724989030161e-06\ngeneration: 14  batch: 400   MSE loss: 8.900381089915754e-07\ngeneration: 15  batch: 0     MSE loss: 4.1349861135131505e-07\ngeneration: 15  batch: 200   MSE loss: 3.26549326246095e-07\ngeneration: 15  batch: 400   MSE loss: 3.319844381621806e-07\ngeneration: 16  batch: 0     MSE loss: 1.6063326313542348e-07\ngeneration: 16  batch: 200   MSE loss: 2.1569556452050165e-07\ngeneration: 16  batch: 400   MSE loss: 2.5400643721695815e-07\ngeneration: 17  batch: 0     MSE loss: 7.991993555833687e-08\ngeneration: 17  batch: 200   MSE loss: 8.242315630013763e-08\ngeneration: 17  batch: 400   MSE loss: 9.084590857355579e-08\ngeneration: 18  batch: 0     MSE loss: 4.878432946497924e-08\ngeneration: 18  batch: 200   MSE loss: 2.3802479987011793e-08\ngeneration: 18  batch: 400   MSE loss: 4.135335984756239e-08\ngeneration: 19  batch: 0     MSE loss: 8.965354680867677e-09\ngeneration: 19  batch: 200   MSE loss: 1.9243438842408978e-08\ngeneration: 19  batch: 400   MSE loss: 1.1302278224434303e-08\ngeneration: 20  batch: 0     MSE loss: 5.0089292713551e-09\ngeneration: 20  batch: 200   MSE loss: 3.921176272569937e-09\ngeneration: 20  batch: 400   MSE loss: 4.728981650714559e-09\ngeneration: 21  batch: 0     MSE loss: 3.098753031949286e-09\ngeneration: 21  batch: 200   MSE loss: 2.4030586409651278e-09\ngeneration: 21  batch: 400   MSE loss: 2.713620883554313e-09\ngeneration: 22  batch: 0     MSE loss: 9.762120001255425e-10\ngeneration: 22  batch: 200   MSE loss: 9.139658474488499e-10\ngeneration: 22  batch: 400   MSE loss: 6.508619754264089e-10\ngeneration: 23  batch: 0     MSE loss: 2.9331595485793116e-10\ngeneration: 23  batch: 200   MSE loss: 5.554421922404629e-10\ngeneration: 23  batch: 400   MSE loss: 2.654713393557273e-10\ngeneration: 24  batch: 0     MSE loss: 1.9251777949591542e-10\ngeneration: 24  batch: 200   MSE loss: 1.535244015249404e-10\ngeneration: 24  batch: 400   MSE loss: 1.4029258310621628e-10\ngeneration: 25  batch: 0     MSE loss: 5.961817844957196e-11\ngeneration: 25  batch: 200   MSE loss: 7.959573278260024e-11\ngeneration: 25  batch: 400   MSE loss: 7.604925023052544e-11\n</code></pre>"},{"location":"function/#the-calculation-error","title":"The calculation error","text":"<pre><code>norm_list_SOR = []\nfor i in range(total_itr+1):\ns_hat, _ = sor_model(i, bs,y)\n'''\n     s_hat the output value from the model of SOR method\n    '''\nerr = (torch.norm(solution.to(device) - s_hat.to(device))**2).item()/(n*bs)\n'''\n    err the norm error from the exact one and the output of SOR method\n    '''\nnorm_list_SOR.append(err)\n</code></pre>"},{"location":"function/#the-result","title":"The result","text":""},{"location":"theory/","title":"Theory","text":"<p>In this section, we will describe the mathematical theory behind iterative methods and their parameters for solving linear systems of equations.</p>"},{"location":"theory/#overview","title":"Overview","text":"<p>Iterative methods have been proposed for solving linear equations. However, due to the numerous parameters and varying performance of these methods, it is necessary to optimize them to improve their convergence rate. Taking inspiration from deep learning tools, the deep unfolding technique has been used to jointly optimize the iterative method's parameters, resulting in accelerated convergence rates.</p> <p>The linear equations can expressed as </p> <p>\\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) </p> <p>where decmpose for \\(\\mathbf{A}\\)</p> <p>\\(\\mathbf{A}=\\mathbf{D}-\\mathbf{U}-\\mathbf{L}.\\)</p> matrix Description \\(\\mathbf{D}\\) diagonal matrix \\(\\mathbf{U}\\) upper triangular matrix \\(\\mathbf{L}\\) lower triangular matrix <p>to solve \\(\\mathbf{x}\\) from the equation above, it can solve in an iteratively way. The accelerated over-relaxation AOR: </p> <p>\\(\\mathbf{x}^{(n+1)}=(\\mathbf{D}-\\beta\\mathbf{L})^{-1}((1-\\alpha)\\mathbf{D}+(\\alpha-\\beta)\\mathbf{L}+\\alpha \\mathbf{U})  \\mathbf{x}^{(n)} \\\\ +(\\mathbf{D}-\\beta\\mathbf{L})^{-1}\\alpha\\mathbf{b}.\\)</p> <p>The successive over-relaxation SOR: </p> <p>\\(\\mathbf{x}^{(n+1)}=(\\mathbf{D}-\\omega\\mathbf{L})^{-1}((1-\\omega)\\mathbf{D}+\\omega \\mathbf{U})\\mathbf{x}^{(n)}+(\\mathbf{D}-\\omega\\mathbf{L})^{-1}\\mathbf{b}.\\) </p> <p>The Gauss\u2013Seidal (GS): </p> <p>\\(\\mathbf{x}^{(n+1)}=(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}\\mathbf{x}^{(n)}+\\mathbf{U}\\mathbf{b}.\\)</p> <p>The Jacobi (JA): </p> <p>\\(\\mathbf{x}^{(n+1)}=\\mathbf{D}^{-1}(\\mathbf{D}-\\mathbf{A})\\mathbf{x}^{n}+\\mathbf{D}^{-1}\\mathbf{b}.\\)</p> <p>The Richardson iteration (RI):</p> <p>\\(\\mathbf{x}^{(n+1)}=(\\mathbf{I}-\\omega \\mathbf{A})\\mathbf{x}^{(n)}+\\omega\\mathbf{b}.\\)</p> <p>To further accelerate these methods, we propose another technique called Chebyshev acceleration method to enhance the convergence rate. The Chebyshev acceleration method can be expressed as:</p> <p>\\(\\mathbf{y}^{(n+1)} = p_{n+1}\\Big(\\gamma (\\mathbf{x}^{(n+1)} - \\mathbf{y}^{(n)}) + (\\mathbf{y}^{(n)}-\\mathbf{y}^{(n-1)}) \\Big)  + \\mathbf{y}^{(n-1)}.\\)</p> Parameter Description \\(\\alpha\\) the relaxation parameter of AOR \\(\\beta\\) acceleration parameter \\(\\omega\\) the relaxation parameter of SOR \\(p_{n+1}\\) under-relaxation \\(\\gamma\\) under-extrapolation"},{"location":"theory/#mathematical-optimization-of-parameters","title":"Mathematical optimization of parameters","text":"<p>The iterative method can be rewrite as different form as </p> <p>\\(\\mathbf{x}^{(n+1)}=\\mathbf{x}^{(n)}\\mathbf{G}+\\mathbf{c}.\\)</p> math vale Description \\(\\rho\\) the spectral radius \\(\\overline{\\mu}\\) The maximum of spectral radius \\(\\underline{\\mu}\\) The minimum of spectral radius \\(\\mathbf{G}\\) The iteration matrix <p>The paramters of iterative emthods has been optmized. The SOR parameter \\(\\omega^{opt}\\):</p> <p>\\(\\omega^{opt}=\\frac{2}{1+\\sqrt{1-\\overline{\\mu}^{2}}}\\)</p> <p>The AOR parameters \\(\\alpha^{opt}, \\beta^{opt}\\): </p> <p>\\(\\alpha^{opt}=\\frac{2}{1+\\sqrt{1-\\overline{\\mu}^{2}}}\\)</p> <p>\\(\\beta^{opt}=\\frac{\\overline{\\mu}^{2}-\\underline{\\mu}^{2}}{\\overline{\\mu}^{2}(1-\\underline{\\mu}^{2})}.\\)</p> <p>The RI parameter $\\omega^{opt} $: </p> <p>\\(\\omega^{opt}=\\frac{2}{\\underline{\\mu}+\\overline{\\mu}}\\) The Chebyshev acceleration method parameters $p_{n+1}, \\gamma $: </p> <p>\\(p_{n+1}^{opt}=\\frac{4}{4-\\rho p_{n}}\\)</p> <p>\\(\\gamma^{opt}=\\frac{2}{2-\\underline{\\mu}+\\overline{\\mu}}\\)</p> <p>All of \\(\\underline{\\mu}, \\overline{\\mu}\\) can calculated by the iteration matrix of Jacobi as </p> <p>\\(\\mu =\\rho(\\mathbf{G}_{JA})=\\rho(\\mathbf{D}^{-1}(\\mathbf{D}-\\mathbf{A})).\\)</p> <p>finding the spectral radius and eigenvectors can be a challenging task despite optimization efforts because these are intrinsic properties of a matrix and are not directly influenced by its parameters. This requires specialized techniques for their computation, and a high spectral radius can still negatively impact the convergence rate of iterative methods</p>"},{"location":"theory/#deep-unfolding-network","title":"Deep unfolding network","text":"<p>A deep learning techniques such as the deep unfolding network can be used to address the challenge of finding optimization parameters without the need for difficult calculations of spectral radius and eigenvectors. This approach has shown promising results and can accelerate the iterative process used to solve linear systems, leading to faster convergence rates.</p> <p>deep unfolding , which takes an iterative algorithm with a fixed number of iterations T, unravels its structure, and adds trainable parameters. These parameters are then trained using deep learning techniques such as loss functions, stochastic gradient descent, and back propagation.</p> <p>The deep unfolding of the iterative algorithm  as follows:</p> <ul> <li>SORNet</li> <li>AORNet</li> <li>RINet</li> <li>ChebySORNet</li> <li>ChebyAORNet</li> </ul> <p>For example SORNet as </p> <p>\\(\\mathbf{x}^{(t+1)}=(\\mathbf{D}-\\omega_{t}\\mathbf{L})^{-1}((1-\\omega_{t})\\mathbf{D}+\\omega_{t} \\mathbf{U})\\mathbf{x}^{(t)}+(\\mathbf{D}-\\omega_{t}\\mathbf{L})^{-1}\\mathbf{b}.\\) </p> <p>here \\(t\\) is the number of training. The trainable internal parameters, such as \\(\\omega_{t}\\) parameter, can be optimized with standard deep learning techniques, i.e., the back propagation and stochastic gradient descent algorithms. The implement training can be express as  </p> math vale Description number of mini batches 500 learning rate of optimizer 0.002 initial values of \\(\\omega\\) 1.1"}]}